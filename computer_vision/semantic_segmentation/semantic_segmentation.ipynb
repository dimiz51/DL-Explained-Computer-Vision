{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import set_model_config, plot_loss\n",
    "from helper import visualize_segmentation_predictions\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "from keras import layers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset and preview basic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Oxford pets\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    'oxford_iiit_pet:3.*.*',\n",
    "    split=['train+test[:50%]', 'test[50%:80%]', 'test[80%:100%]'],\n",
    "    with_info=True)\n",
    "\n",
    "# Access and print dataset information\n",
    "print(\"Oxford pets dataset information:\")\n",
    "print(f\"Number of classes: {info.features['label'].num_classes}\")\n",
    "print(f\"Class names: {info.features['label'].names}\")\n",
    "print(f\"Number of training examples: {info.splits['train'].num_examples}\")\n",
    "print(f\"Dataset splits: {list(info.splits.keys())}\")\n",
    "print(f\"Dataset description: {info.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model config\n",
    "config = set_model_config(model_name = 'oxford_unet')\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentations to both mask and images for trainset\n",
    "def augmentations(image, mask):\n",
    "    image = tf.image.resize(image , (224,224))\n",
    "    mask = tf.image.resize(mask, (224,224))\n",
    "\n",
    "    # Random horizontal flip for data augmentation\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "    return image, tf.cast(mask, dtype = tf.uint8)\n",
    "\n",
    "# Resize mask/images pre-process for inference\n",
    "def resize_inference(image, mask):\n",
    "    image = tf.image.resize(image , (224,224))\n",
    "    mask = tf.image.resize(mask, (224,224))\n",
    "\n",
    "    return image, tf.cast(mask, dtype = tf.uint8)\n",
    "\n",
    "# Convert to binary\n",
    "def binary_mask(mask):\n",
    "    mask = tf.cast(mask, dtype = tf.int32)\n",
    "    converted_mask = tf.where(tf.equal(mask, 1), 1, mask)\n",
    "    converted_mask = tf.where(tf.equal(mask, 2), 0, converted_mask)\n",
    "    converted_mask = tf.where(tf.equal(mask, 3), 0, converted_mask)\n",
    "    return tf.cast(converted_mask, dtype = tf.uint8)\n",
    "\n",
    "# Pre-process trainset\n",
    "def preprocess_train(element):\n",
    "    image = tf.image.convert_image_dtype(element['image'], tf.float32)\n",
    "    segmentation_mask = tf.image.convert_image_dtype(element['segmentation_mask'], tf.uint8)\n",
    "    segmentation_mask = binary_mask(segmentation_mask)\n",
    "\n",
    "    # Apply augmentations\n",
    "    image, segmentation_mask = augmentations(image, segmentation_mask)\n",
    "\n",
    "    return image, segmentation_mask\n",
    "\n",
    "# Pre-process val/test sets\n",
    "def preprocess_val_test(element):\n",
    "    image = tf.image.convert_image_dtype(element['image'], tf.float32)\n",
    "    segmentation_mask = tf.image.convert_image_dtype(element['segmentation_mask'], tf.uint8)\n",
    "    segmentation_mask = binary_mask(segmentation_mask)\n",
    "\n",
    "\n",
    "    # Resize image and mask for inference\n",
    "    image, segmentation_mask = resize_inference(image, segmentation_mask)\n",
    "\n",
    "    return image, segmentation_mask\n",
    "\n",
    "# Pre-processing and augmentations for training set\n",
    "train_dataset = train_ds.map(preprocess_train, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(config['batch_size'])\n",
    "train_dataset = train_dataset.shuffle(buffer_size=config['batch_size'] * 6)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Inference pre-process for val/test datasets\n",
    "validation_dataset = val_ds.map(preprocess_val_test, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(config['batch_size'])\n",
    "validation_dataset = validation_dataset.shuffle(buffer_size=config['batch_size'] * 6)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = test_ds.map(preprocess_val_test)\n",
    "test_dataset = test_dataset.batch(config['batch_size'])\n",
    "test_dataset = test_dataset.shuffle(buffer_size=8 * 6)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a few samples from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=3):\n",
    "    # Take the first 'num_samples' samples from the dataset\n",
    "    sample_dataset = dataset.take(num_samples)\n",
    "\n",
    "    # Create a subplot for displaying images in the grid\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "\n",
    "    # Iterate through the samples dataset\n",
    "    for i, batch in enumerate(sample_dataset):\n",
    "        image = batch['image'].numpy().astype(int)\n",
    "        segmentation_mask = batch['segmentation_mask'].numpy().astype(int)\n",
    "\n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(image)\n",
    "        axes[i, 0].set_title(f'Original Image - Sample {i + 1}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot segmentation mask\n",
    "        axes[i, 1].imshow(segmentation_mask[:, :, 0], cmap='gray')\n",
    "        axes[i, 1].set_title(f'Segmentation Mask - Sample {i + 1}')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "  visualize_samples(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check pre-processed segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_dataset.take(1)\n",
    "\n",
    "for i, masks in samples:\n",
    "  print(\"Batch shapes| Images: {i.shape}  | Masks: {masks.shape}\")\n",
    "\n",
    "  # Get unique values\n",
    "  unique_values = tf.unique(tf.reshape(masks, [-1]))\n",
    "  # Print unique values\n",
    "  print(\"Unique values in mask:\", unique_values.y.numpy())\n",
    "\n",
    "  for x in range(len(masks)):\n",
    "    mask = masks[x].numpy()\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(i[x])\n",
    "    plt.title('Image')\n",
    "\n",
    "    # Plot the mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title('Mask')\n",
    "\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Unet with pre-trained encoder\n",
    "def unet_model(img_size, num_classes):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=img_size, include_top=False)\n",
    "\n",
    "    names = ['block_1_expand_relu', 'block_3_expand_relu', 'block_6_expand_relu',\n",
    "         'block_13_expand_relu', 'block_16_expand_relu']\n",
    "    encoder_layers = [base_model.get_layer(name).output for name in names]\n",
    "\n",
    "    down_sample = tf.keras.Model(base_model.input, encoder_layers)\n",
    "    down_sample.trainable = False\n",
    "\n",
    "    # Downsampling through the model\n",
    "    inputs = keras.Input(shape = img_size)\n",
    "    x = inputs\n",
    "    skips = down_sample(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    up_stack = [layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "                layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "              ]\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    conv10 = layers.Conv2DTranspose(1, 3, strides=2 , padding='same')(x)\n",
    "\n",
    "    # Create the UNet model\n",
    "    model = keras.models.Model(inputs=inputs, outputs=[conv10])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "                  loss=keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "                  metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1],\n",
    "                                                      name = 'IoU'),\n",
    "                           'accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = unet_model((224, 224, 3), config['n_classes'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set training callbacks and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate scheduler to progressively reduce the learning rate as learning plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                 patience=5, min_lr=0.0001)\n",
    "\n",
    "# Always save the best model\n",
    "saving_cb = ModelCheckpoint(\n",
    "    filepath='./trained_models/oxford_segmentation/best_model.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss', \n",
    "    mode='min',  \n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with class weights and EarlyStopping\n",
    "history = model.fit(train_dataset, epochs=2, validation_data=validation_dataset, callbacks = [lr_scheduler, saving_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "with plt.style.context('dark_background'):\n",
    "    plot_loss(history, model_type= 'segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    visualize_segmentation_predictions(dataset = test_dataset, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./trained_models/oxford_segmentation/oxford_unet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
