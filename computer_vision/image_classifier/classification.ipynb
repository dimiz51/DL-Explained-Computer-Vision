{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from helper import visualize_classification_image_samples, visualize_classification_predictions, plot_confusion_matrix\n",
    "from helper import fast_benchmark, set_model_config\n",
    "from helper import plot_loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global variables\n",
    "model_config = set_model_config('cifar_10')\n",
    "\n",
    "'''Create a random seed generator for randomized TF ops'''\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and show information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Access and print dataset information\n",
    "print(\"CIFAR-10 dataset information:\")\n",
    "print(f\"Number of classes: {ds_info.features['label'].num_classes}\")\n",
    "print(f\"Class names: {ds_info.features['label'].names}\")\n",
    "print(f\"Number of training examples: {ds_info.splits['train'].num_examples}\")\n",
    "print(f\"Dataset splits: {list(ds_info.splits.keys())}\")\n",
    "print(f\"Dataset description: {ds_info.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate the dataset and visualize some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate the dataset \n",
    "iterator = iter(ds_train.take(3))\n",
    "\n",
    "for i in range(3):\n",
    "    image, label = next(iterator)\n",
    "    print(f'Sample {i} tensor shape: {image.shape}')\n",
    "    print(f'Sample {i} class label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a grid of samples from the training set\n",
    "with plt.style.context('dark_background'):\n",
    "    visualize_classification_image_samples(ds_train, 10, ds_info, g_shape=(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply pre-processing and augmentations before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def normalize_image(image: tf.Tensor,label: tf.Tensor)-> (tf.Tensor, tf.Tensor):\n",
    "    '''Define a normalization function that rescales \n",
    "    the pixel values from [0,255] uint8 to float32 [0,1]\n",
    "    '''\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Augmentations\n",
    "def augment_image(image_label: tuple, seed)-> (tf.Tensor, tf.Tensor):\n",
    "    '''Apply basic augmentations on the training dataset samples\n",
    "    in order to induce extra variance to our dataset. This can help\n",
    "    our model generalize in a better way. Augmentations applied are \n",
    "    random horizontal flip, random crop and random rotation by 45 degrees.'''\n",
    "\n",
    "    image, label = image_label\n",
    "    new_seed = tf.random.split(seed, num=1)[0, :]\n",
    "    image = tf.image.stateless_random_flip_left_right(image, new_seed)\n",
    "\n",
    "    angle = tf.random.uniform(shape=(), minval=-45, maxval=45, dtype=tf.float32)\n",
    "    image = tf.image.rot90(image, k=tf.cast(angle / 90, dtype=tf.int32))\n",
    "\n",
    "    image = tf.image.stateless_random_crop(value= image, size= (32,32,3), seed= new_seed)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def random_wrapper(image: tf.Tensor, label: tf.Tensor)-> (tf.Tensor, tf.Tensor):\n",
    "    '''Wrapper function for our augmentations to generate a new random \n",
    "    seed on each call. This way we can indeed have random augmentations \n",
    "    for each sample.'''\n",
    "\n",
    "    seed = rng.make_seeds(2)[0]\n",
    "    image, label = augment_image((image, label), seed)\n",
    "    return image,label\n",
    "\n",
    "# Generate the validation set from the training set\n",
    "validation_size = int(model_config['val_size'] * ds_info.splits['train'].num_examples) \n",
    "\n",
    "# Create the validation dataset\n",
    "ds_val = ds_train.take(validation_size)\n",
    "ds_train = ds_train.skip(validation_size)\n",
    "\n",
    "# Print the number of examples in the training and validation sets\n",
    "print(\"Number of images in training set:\", ds_info.splits['train'].num_examples - validation_size)\n",
    "print(\"Number of images in validation set:\", validation_size)\n",
    "print(\"Number of images in test set:\", ds_info.splits['test'].num_examples)\n",
    "\n",
    "# Pipelining pre-processing, augmentations, batching and caching of the training dataset.\n",
    "''''Order of things: 1. Caching Dataset into memory\n",
    "                     2. Shuffling dataset to introduce randomness on each training epoch\n",
    "                     3. Apply augmentations to the shuffled dataset using parallel mapping\n",
    "                     4. Batch dataset and normalize the images using vectorized mapping \n",
    "                     5. Prefetch dataset for performance optimization'''\n",
    "\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples, reshuffle_each_iteration=True)\n",
    "ds_train = ds_train.map(random_wrapper, \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.batch(model_config['batch_size']).map(normalize_image)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Prepare the validation set\n",
    "ds_val = ds_val.cache()\n",
    "ds_val = ds_val.shuffle(validation_size, reshuffle_each_iteration=True)\n",
    "ds_val = ds_val.batch(model_config['batch_size']).map(normalize_image)\n",
    "ds_val = ds_val.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Prepare the test set \n",
    "ds_test = ds_test.batch(model_config['batch_size']).map(normalize_image)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.shuffle(ds_info.splits['test'].num_examples)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run benchmarks on the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Benchmark our training dataset for 2 epochs to test our input pipeline's efficiency \n",
    "    using parallel mapping \"\"\" \n",
    "print('Run before caching training dataset...')\n",
    "fast_benchmark(ds_train)\n",
    "print('Second run after caching training dataset...')\n",
    "fast_benchmark(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Benchmark our val dataset for 2 epochs to test our input pipeline's efficiency \n",
    "    using vectorized mapping \"\"\" \n",
    "print('Run before caching validation dataset...')\n",
    "fast_benchmark(ds_val)\n",
    "print('Second run after caching validation dataset...')\n",
    "fast_benchmark(ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple image classification model using the Sequential API from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using the Keras Sequential API\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define our model's architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', \n",
    "           data_format= 'channels_last', \n",
    "           input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    # Conv2D(256, (3, 3), activation='relu'),\n",
    "    # MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    # Dense(128, activation='relu'),\n",
    "    Dense(model_config['n_classes'], activation = None)\n",
    "], name='cifar10_model_1.0')\n",
    "\n",
    "# Compile and configure the model for training\n",
    "\n",
    "# NOTE: Feel free to replace this with your own optimizer\n",
    "optimizer = Adam(learning_rate=model_config['learning_rate'])\n",
    "\n",
    "# Set Early Stopping strategy after 5 epochs of no improvement for validation set\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='Accuracy'),\n",
    "                       tf.keras.metrics.SparseTopKCategoricalAccuracy(name= 'TopKAccuracy')]\n",
    "              )\n",
    "\n",
    "# Get a detailed view of how the defined model looks\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model, inspect the loss curve during training and experiment \n",
    "    with different architectures. Take inspiration from the idea below or \n",
    "    try something one of your own...!\n",
    "    \n",
    "    Experiment idea: 1. Redifine the model's architecture with an extra convolutional\n",
    "                     or dense layers and compare the models \n",
    "                     2. Try setting a different learning rate and optimizer in the model's settings\n",
    "                     \"\"\"\n",
    "# Train model and plot losses\n",
    "history = model.fit(ds_train, epochs= int(model_config['training_epochs']), \n",
    "                    validation_data= ds_val, \n",
    "                    callbacks = [callback])\n",
    "\n",
    "# Plot with dark backgorund\n",
    "with plt.style.context('dark_background'):\n",
    "    plot_loss(history, model_type = 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log metrics and print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate the model on the test set\n",
    "   and inspect the results. \n",
    "   How reliable is our model? How do the test metrics compare with your training \n",
    "   or validation metrics ? \"\"\"\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "evaluation_result = model.evaluate(ds_test, verbose= 0)\n",
    "\n",
    "# Print and log the evaluation metrics\n",
    "test_metrics = evaluation_result[1:]\n",
    "\n",
    "# Create a PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Metric\", \"Value\"]\n",
    "\n",
    "# Add rows\n",
    "table.add_row([\"Accuracy\", f\"{test_metrics[0]:.4f}\"])\n",
    "table.add_row([\"TopKAccuracy\", f\"{test_metrics[1]:.4f}\"])\n",
    "\n",
    "# Print the table\n",
    "print(\"--------Test dataset metrics--------\")\n",
    "print(table)\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Plot confusion matrix with default background\n",
    "with plt.style.context('default'):\n",
    "    plot_confusion_matrix(model, ds_test, ds_info.features['label'].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained model and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run inference on the test set for 10 samples and visualize \n",
    "    predictions versus true labels \"\"\"\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "test_iterator = iter(ds_test.take(1))\n",
    "single_batch = next(test_iterator)\n",
    "images = single_batch[0][:15]\n",
    "true_labels = list(single_batch[1][:15].numpy())\n",
    "\n",
    "# Load a trained model and visualize predictions\n",
    "trained_model = load_model('computer_vision/trained_models/cifar10_model')\n",
    "predictions = trained_model.predict(images, verbose= 0)\n",
    "\n",
    "# Plot with dark background\n",
    "with plt.style.context('dark_background'):\n",
    "    visualize_classification_predictions(images, true_labels, predictions, ds_info, num_samples= 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
